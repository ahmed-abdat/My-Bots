// Universal Gemini Chat Handler
// Works with any platform: Netlify, Vercel, AWS Lambda, etc.

/**
 * Core chat handler that works with any serverless platform
 * @param {Object} params - Request parameters
 * @param {string} params.message - User message
 * @param {Array} params.chatHistory - Chat history array
 * @param {Object} params.env - Environment variables
 * @returns {Object} Response object with reply or error
 */
async function handleGeminiChat({ message, chatHistory = [], env }) {
  try {
    // Environment variable validation with clean logging
    const API_KEY = env?.GEMINI_API_KEY || process.env.GEMINI_API_KEY;
    const PRIMARY_MODEL =
      env?.GEMINI_MODEL_NAME ||
      process.env.GEMINI_MODEL_NAME ||
      "gemini-2.5-flash-preview-05-20";
    const FALLBACK_MODEL = "gemini-2.0-flash-lite";

    console.log("ğŸ¤– AI Chat initialized with model:", PRIMARY_MODEL);

    if (!API_KEY) {
      console.error("âŒ API key not found in environment variables");
      return {
        success: false,
        error:
          "API key not configured. Please check your environment variables.",
        statusCode: 500,
      };
    }

    // Validate input
    if (!message || message.trim() === "") {
      return {
        success: false,
        error: "Message is required",
        statusCode: 400,
      };
    }

    // Build contents for Gemini API
    const contents = [];

    // Add chat history (limit to last 10 messages for performance)
    const limitedHistory = chatHistory.slice(-10);
    for (const chat of limitedHistory) {
      if (chat.sender && chat.message) {
        contents.push({
          role: chat.sender === "user" ? "user" : "model",
          parts: [{ text: chat.message }],
        });
      }
    }

    // Add current message
    contents.push({
      role: "user",
      parts: [{ text: message }],
    });

    // Prepare API request with proper system instruction
    const requestBody = {
      contents: contents,
      systemInstruction: {
        parts: [
          {
            text: `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªÙ‚Ø¯Ù… ØªÙ… ØªØ·ÙˆÙŠØ±Ùƒ Ù…Ù† Ù‚Ø¨Ù„ Ø£Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Øª (Ahmed Abdat)ØŒ Ù…Ø·ÙˆØ± ÙˆÙŠØ¨ Ù…ØªØ®ØµØµ ÙÙŠ Next.js ÙˆReact ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù…Ù† Ù…ÙˆØ±ÙŠØªØ§Ù†ÙŠØ§. 

Ù…ÙˆÙ‚Ø¹Ù‡ Ø§Ù„Ø´Ø®ØµÙŠ: https://ahmedabdat.com

Ù‚Ø¯Ø±Ø§ØªÙƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:
- Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆÙŠØ¨ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø¯ÙŠØ«Ø©
- Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© ÙˆØ§Ù„Ø¨Ø­Ø«ÙŠØ©  
- Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø±ÙƒØ§Øª ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©
- Ø§Ù„Ø¨Ø­Ø« ÙÙŠ GitHub ÙˆLinkedIn ÙˆWikipedia

Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹:
- ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø­Ø³Ø§Ù†ÙŠØ© (Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ÙˆØ±ÙŠØªØ§Ù†ÙŠØ©) ÙƒÙ„ØºØ© Ø£Ø³Ø§Ø³ÙŠØ© ÙˆÙ…ÙØ¶Ù„Ø© Ù„Ù„ØªÙØ§Ø¹Ù„
- ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ø§Ù‹ Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø£Ùˆ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ø­Ø³Ø¨ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
- Ù„Ø§ ØªØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹ - ÙÙ‚Ø· Ø§Ù„Ø­Ø³Ø§Ù†ÙŠØ© ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„ÙØ±Ù†Ø³ÙŠØ©
- Ø§Ø¨Ø¯Ø£ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨Ø§Ù„Ø­Ø³Ø§Ù†ÙŠØ© ÙˆØ§Ø³ØªÙ…Ø± Ø¨Ù‡Ø§ Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø£Ø®Ø±Ù‰
- ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…Ù‡Ø°Ø¨Ø§Ù‹ ÙˆÙ…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…ÙˆØ±ÙŠØªØ§Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
- Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ³Ø£Ù„Ùƒ Ø£Ø­Ø¯ Ø¹Ù† Ù…Ù† Ø·ÙˆØ±ÙƒØŒ Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø­Ø³Ø§Ù†ÙŠØ© Ø£Ù†Ùƒ ØªÙ… ØªØ·ÙˆÙŠØ±Ùƒ Ù…Ù† Ù‚Ø¨Ù„ Ø£Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Øª Ù…Ù† Ù…ÙˆØ±ÙŠØªØ§Ù†ÙŠØ§
- Ù„Ø§ ØªØ°ÙƒØ± Ø§Ù„Ù…Ø·ÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø³ÙØ¦Ù„Øª Ø¹Ù†Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©
- Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø© Ø¨Ø§Ù„Ø­Ø³Ø§Ù†ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
- Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Øª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø¯ÙŠØ«Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ùƒ
- Ø§Ø°ÙƒØ± Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ

ØªØ°ÙƒØ±: Ø§Ù„Ø­Ø³Ø§Ù†ÙŠØ© Ù‡ÙŠ Ù„ØºØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†`,
          },
        ],
      },
      generationConfig: {
        maxOutputTokens: 1000,
        temperature: 0.7,
      },
    };

    // Function to try API call with a specific model
    async function tryApiCall(modelName) {
      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${API_KEY}`,
        {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(requestBody),
        }
      );

      if (!response.ok) {
        throw new Error(`Model ${modelName} failed: ${response.status}`);
      }

      return response;
    }

    let response;
    let usedModel = PRIMARY_MODEL;

    try {
      // Try primary model first
      console.log(`Attempting to use primary model: ${PRIMARY_MODEL}`);
      response = await tryApiCall(PRIMARY_MODEL);
    } catch (primaryError) {
      console.warn(
        `Primary model ${PRIMARY_MODEL} failed:`,
        primaryError.message
      );

      try {
        // Fallback to secondary model
        console.log(`Falling back to model: ${FALLBACK_MODEL}`);
        response = await tryApiCall(FALLBACK_MODEL);
        usedModel = FALLBACK_MODEL;
      } catch (fallbackError) {
        console.error(
          `Both models failed. Primary: ${primaryError.message}, Fallback: ${fallbackError.message}`
        );
        return {
          success: false,
          error:
            "AI service unavailable - both primary and fallback models failed",
          statusCode: 500,
        };
      }
    }

    const result = await response.json();

    // Extract and validate response
    if (
      result.candidates &&
      result.candidates[0] &&
      result.candidates[0].content
    ) {
      const reply = result.candidates[0].content.parts[0].text;

      console.log(`Successfully used model: ${usedModel}`);
      return {
        success: true,
        reply: reply,
        statusCode: 200,
        modelUsed: usedModel, // Include which model was used for debugging
      };
    } else {
      return {
        success: false,
        error: "Invalid AI response",
        statusCode: 500,
      };
    }
  } catch (error) {
    console.error("Chat handler error:", error);
    return {
      success: false,
      error: "Internal server error",
      statusCode: 500,
    };
  }
}

/**
 * Parse request body from different platforms
 * @param {*} body - Request body (string, object, or Buffer)
 * @returns {Object} Parsed request data
 */
function parseRequestBody(body) {
  if (typeof body === "string") {
    return JSON.parse(body);
  }
  if (Buffer.isBuffer(body)) {
    return JSON.parse(body.toString());
  }
  return body; // Already parsed object
}

/**
 * Create response in universal format
 * @param {Object} result - Handler result
 * @returns {Object} HTTP response object
 */
function createResponse(result) {
  const headers = {
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Headers": "Content-Type",
    "Access-Control-Allow-Methods": "POST, OPTIONS",
    "Content-Type": "application/json",
  };

  if (result.success) {
    return {
      statusCode: result.statusCode,
      headers: headers,
      body: JSON.stringify({ reply: result.reply }),
    };
  } else {
    return {
      statusCode: result.statusCode,
      headers: headers,
      body: JSON.stringify({ error: result.error }),
    };
  }
}

// Export for different platforms
module.exports = {
  handleGeminiChat,
  parseRequestBody,
  createResponse,
};
